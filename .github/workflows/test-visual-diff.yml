name: Test Visual Diff System

on:
  pull_request:
    types: [opened, synchronize]
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      test_scenario:
        description: 'Test scenario to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - baseline
          - changed
          - mixed

permissions:
  contents: write
  issues: write
  pull-requests: write

jobs:
  test-visual-diff-system:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        scenario: ${{ github.event.inputs.test_scenario == 'all' && fromJson('["baseline", "changed", "mixed"]') || (github.event.inputs.test_scenario && fromJson(format('["{0}"]', github.event.inputs.test_scenario))) || fromJson('["baseline", "changed", "mixed"]') }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Setup Python for test execution
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install Pillow numpy
    
    - name: Create necessary directories
      run: |
        mkdir -p outputs
        mkdir -p diffs
        mkdir -p golden
    
    - name: Generate test images for scenario
      run: |
        export TEST_SCENARIO="${{ matrix.scenario }}"
        echo "Testing visual diff system with scenario: $TEST_SCENARIO"
        python generate_test_images.py
    
    - name: Verify test images were generated
      run: |
        if [ ! -d "outputs" ] || [ -z "$(ls -A outputs/ 2>/dev/null)" ]; then
          echo "âŒ Test image generation failed - no outputs found"
          exit 1
        fi
        
        IMAGE_COUNT=$(ls -1 outputs/*.png 2>/dev/null | wc -l)
        echo "âœ… Generated $IMAGE_COUNT test images for scenario: ${{ matrix.scenario }}"
        ls -la outputs/
    
    - name: Test visual diff workflow with generated images
      run: |
        # Install NVIDIA FLIP for image comparison
        pip install flip-evaluator
        
        # Initialize test results
        EXPECTED_RESULT=""
        case "${{ matrix.scenario }}" in
          "baseline")
            EXPECTED_RESULT="no_changes"
            echo "Expected result: All images should match (no changes detected)"
            ;;
          "changed")
            EXPECTED_RESULT="changes"
            echo "Expected result: All images should differ (changes detected)"
            ;;
          "mixed")
            EXPECTED_RESULT="mixed"
            echo "Expected result: Some images match, some differ (mixed results)"
            ;;
        esac
        
        # Run the same comparison logic as the visual-diff workflow
        CHANGED_IMAGES=""
        CHANGES_DETECTED=false
        
        echo "# Test Results for Scenario: ${{ matrix.scenario }}" > test_report.md
        echo "" >> test_report.md
        echo "| File | Status | FLIP Mean Error | Expected | Result |" >> test_report.md
        echo "|------|--------|-----------------|----------|--------|" >> test_report.md
        
        # Process each output image
        for output_file in outputs/*.png; do
          if [ ! -f "$output_file" ]; then
            continue
          fi
          
          filename=$(basename "$output_file")
          golden_file="golden/$filename"
          
          echo "Testing $filename..."
          
          if [ ! -f "$golden_file" ]; then
            # New image - should only happen in specific test scenarios
            echo "| \`$filename\` | ðŸ†• New | N/A | New image | âœ… |" >> test_report.md
            CHANGED_IMAGES="$CHANGED_IMAGES $filename"
            CHANGES_DETECTED=true
          else
            # Compare using FLIP
            git lfs pull --include "$golden_file"
            
            basename_no_ext=$(basename "$filename" .png)
            diff_basename="diff_${basename_no_ext}"
            
            flip_output=$(flip -r "$golden_file" -t "$output_file" -d diffs -b "$diff_basename" -v 2 -txt 2>&1)
            flip_exit_code=$?
            
            if [ $flip_exit_code -ne 0 ]; then
              echo "| \`$filename\` | âŒ Error | N/A | FLIP error | âŒ |" >> test_report.md
              continue
            fi
            
            mean_error=$(echo "$flip_output" | grep "Mean:" | awk '{print $2}')
            is_different=$(echo "$mean_error" | awk '{if ($1 > 0.001) print "yes"; else print "no"}')
            
            if [ "$is_different" = "yes" ]; then
              echo "| \`$filename\` | ðŸ”„ Changed | $mean_error | Changed | âœ… |" >> test_report.md
              CHANGED_IMAGES="$CHANGED_IMAGES $filename"
              CHANGES_DETECTED=true
            else
              echo "| \`$filename\` | âœ… Passed | $mean_error | No change | âœ… |" >> test_report.md
            fi
          fi
        done
        
        # Validate results match expectations
        echo "" >> test_report.md
        echo "## Test Validation" >> test_report.md
        echo "" >> test_report.md
        
        TEST_PASSED=true
        case "$EXPECTED_RESULT" in
          "no_changes")
            if [ "$CHANGES_DETECTED" = "false" ]; then
              echo "âœ… **Test PASSED**: No changes detected as expected for baseline scenario" >> test_report.md
            else
              echo "âŒ **Test FAILED**: Changes detected but none were expected for baseline scenario" >> test_report.md
              TEST_PASSED=false
            fi
            ;;
          "changes")
            if [ "$CHANGES_DETECTED" = "true" ]; then
              echo "âœ… **Test PASSED**: Changes detected as expected for changed scenario" >> test_report.md
            else
              echo "âŒ **Test FAILED**: No changes detected but changes were expected for changed scenario" >> test_report.md
              TEST_PASSED=false
            fi
            ;;
          "mixed")
            if [ "$CHANGES_DETECTED" = "true" ]; then
              echo "âœ… **Test PASSED**: Mixed results detected as expected for mixed scenario" >> test_report.md
            else
              echo "âŒ **Test FAILED**: No changes detected but mixed results were expected" >> test_report.md
              TEST_PASSED=false
            fi
            ;;
        esac
        
        echo "" >> test_report.md
        echo "**Scenario**: ${{ matrix.scenario }}" >> test_report.md
        echo "**Expected**: $EXPECTED_RESULT" >> test_report.md
        echo "**Actual**: $([ "$CHANGES_DETECTED" = "true" ] && echo "changes_detected" || echo "no_changes")" >> test_report.md
        echo "**Result**: $([ "$TEST_PASSED" = "true" ] && echo "PASSED" || echo "FAILED")" >> test_report.md
        
        cat test_report.md
        
        if [ "$TEST_PASSED" = "false" ]; then
          echo "âŒ Test validation failed for scenario: ${{ matrix.scenario }}"
          exit 1
        else
          echo "âœ… Test validation passed for scenario: ${{ matrix.scenario }}"
        fi
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: visual-diff-test-results-${{ matrix.scenario }}
        path: |
          outputs/
          diffs/
          golden/
          test_report.md
        retention-days: 7

  summarize-tests:
    runs-on: ubuntu-latest
    needs: test-visual-diff-system
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v4
      with:
        pattern: visual-diff-test-results-*
        path: ./test-results
        merge-multiple: true
    
    - name: Generate test summary
      run: |
        echo "# Visual Diff System Test Summary" > test_summary.md
        echo "" >> test_summary.md
        echo "**Test Run**: $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> test_summary.md
        echo "" >> test_summary.md
        
        # Check if all tests passed
        ALL_PASSED=true
        if [ "${{ needs.test-visual-diff-system.result }}" != "success" ]; then
          ALL_PASSED=false
        fi
        
        if [ "$ALL_PASSED" = "true" ]; then
          echo "## âœ… All Tests Passed" >> test_summary.md
          echo "" >> test_summary.md
          echo "The visual diff system is working correctly across all test scenarios:" >> test_summary.md
          echo "- **Baseline**: Correctly detects no changes when images match" >> test_summary.md
          echo "- **Changed**: Correctly detects changes when images differ" >> test_summary.md
          echo "- **Mixed**: Correctly handles mixed scenarios" >> test_summary.md
        else
          echo "## âŒ Some Tests Failed" >> test_summary.md
          echo "" >> test_summary.md
          echo "The visual diff system has issues that need to be addressed." >> test_summary.md
          echo "Check the individual test results for details." >> test_summary.md
        fi
        
        echo "" >> test_summary.md
        echo "## Test Scenarios Results" >> test_summary.md
        echo "" >> test_summary.md
        
        # Process any test reports found
        for report in test-results/test_report.md; do
          if [ -f "$report" ]; then
            echo "Found test report: $report"
            cat "$report" >> test_summary.md
            echo "" >> test_summary.md
          fi
        done
        
        echo "" >> test_summary.md
        echo "---" >> test_summary.md
        echo "**Note**: This workflow tests the visual diff system itself. The production visual-diff workflow is separate and ready for integration with external CI systems." >> test_summary.md
        
        cat test_summary.md
        
        # Fail the workflow if tests failed
        if [ "$ALL_PASSED" = "false" ]; then
          echo "âŒ Visual diff system tests failed"
          exit 1
        else
          echo "âœ… Visual diff system tests passed"
        fi
    
    - name: Upload test summary
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: visual-diff-test-summary
        path: test_summary.md
        retention-days: 30
    
    - name: Comment PR with test results
      if: always() && github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read the test summary
          let report;
          try {
            report = fs.readFileSync('test_summary.md', 'utf8');
          } catch (error) {
            report = "## âŒ Visual Diff System Test Summary\n\nTest summary could not be generated due to an error.\n\n**Error:** " + error.message;
          }
          
          // Add header to distinguish from production visual diff results
          const testReport = "## ðŸ§ª Visual Diff System Test Results\n\n" + 
            "> This is automated testing of the visual diff system itself, not production visual regression testing.\n\n" + 
            report;
          
          // Clean up old test result comments first
          const comments = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number
          });
          
          const botTestComments = comments.data.filter(comment => 
            comment.user.type === 'Bot' && 
            comment.body.includes('Visual Diff System Test Results')
          );
          
          // Delete old test comments to avoid confusion
          for (const comment of botTestComments) {
            try {
              await github.rest.issues.deleteComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: comment.id
              });
              console.log(`Deleted old test comment: ${comment.id}`);
            } catch (error) {
              console.log(`Failed to delete comment ${comment.id}: ${error.message}`);
            }
          }
          
          // Always create a new comment with test results
          await github.rest.issues.createComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
            body: testReport
          });