name: Test Visual Diff System

on:
  pull_request:
    types: [opened, synchronize]
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      test_scenario:
        description: 'Test scenario to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - baseline
          - changed
          - mixed

permissions:
  contents: write
  issues: write
  pull-requests: write

jobs:
  test-visual-diff-system:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        scenario: ${{ github.event.inputs.test_scenario == 'all' && fromJson('["baseline", "changed", "mixed"]') || (github.event.inputs.test_scenario && fromJson(format('["{0}"]', github.event.inputs.test_scenario))) || fromJson('["baseline", "changed", "mixed"]') }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Setup Python for test execution
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install Pillow numpy
    
    - name: Create necessary directories
      run: |
        mkdir -p outputs
        mkdir -p diffs
        mkdir -p golden
    
    - name: Generate test images for scenario
      run: |
        export TEST_SCENARIO="${{ matrix.scenario }}"
        echo "Testing visual diff system with scenario: $TEST_SCENARIO"
        python generate_test_images.py
    
    - name: Verify test images were generated
      run: |
        if [ ! -d "outputs" ] || [ -z "$(ls -A outputs/ 2>/dev/null)" ]; then
          echo "❌ Test image generation failed - no outputs found"
          exit 1
        fi
        
        IMAGE_COUNT=$(ls -1 outputs/*.png 2>/dev/null | wc -l)
        echo "✅ Generated $IMAGE_COUNT test images for scenario: ${{ matrix.scenario }}"
        ls -la outputs/
    
    - name: Test visual diff workflow with generated images
      run: |
        # Install NVIDIA FLIP for image comparison
        pip install flip-evaluator
        
        # Initialize test results
        EXPECTED_RESULT=""
        case "${{ matrix.scenario }}" in
          "baseline")
            EXPECTED_RESULT="no_changes"
            echo "Expected result: All images should match (no changes detected)"
            ;;
          "changed")
            EXPECTED_RESULT="changes"
            echo "Expected result: All images should differ (changes detected)"
            ;;
          "mixed")
            EXPECTED_RESULT="mixed"
            echo "Expected result: Some images match, some differ (mixed results)"
            ;;
        esac
        
        # Run the same comparison logic as the visual-diff workflow
        CHANGED_IMAGES=""
        CHANGES_DETECTED=false
        
        # Create unique test report filename for this scenario
        TEST_REPORT="test_report_${{ matrix.scenario }}.md"
        echo "# Test Results for Scenario: ${{ matrix.scenario }}" > "$TEST_REPORT"
        echo "" >> "$TEST_REPORT"
        echo "| File | Status | FLIP Mean Error | Expected | Result |" >> "$TEST_REPORT"
        echo "|------|--------|-----------------|----------|--------|" >> "$TEST_REPORT"
        
        # Process each output image
        for output_file in outputs/*.png; do
          if [ ! -f "$output_file" ]; then
            continue
          fi
          
          filename=$(basename "$output_file")
          golden_file="golden/$filename"
          
          echo "Testing $filename..."
          
          if [ ! -f "$golden_file" ]; then
            # New image - should only happen in specific test scenarios
            echo "| \`$filename\` | 🆕 New | N/A | New image | ✅ |" >> "$TEST_REPORT"
            CHANGED_IMAGES="$CHANGED_IMAGES $filename"
            CHANGES_DETECTED=true
          else
            # Compare using FLIP
            git lfs pull --include "$golden_file"
            
            basename_no_ext=$(basename "$filename" .png)
            diff_basename="diff_${basename_no_ext}"
            
            flip_output=$(flip -r "$golden_file" -t "$output_file" -d diffs -b "$diff_basename" -v 2 -txt 2>&1)
            flip_exit_code=$?
            
            if [ $flip_exit_code -ne 0 ]; then
              echo "| \`$filename\` | ❌ Error | N/A | FLIP error | ❌ |" >> "$TEST_REPORT"
              continue
            fi
            
            mean_error=$(echo "$flip_output" | grep "Mean:" | awk '{print $2}')
            is_different=$(echo "$mean_error" | awk '{if ($1 > 0.001) print "yes"; else print "no"}')
            
            if [ "$is_different" = "yes" ]; then
              echo "| \`$filename\` | 🔄 Changed | $mean_error | Changed | ✅ |" >> "$TEST_REPORT"
              CHANGED_IMAGES="$CHANGED_IMAGES $filename"
              CHANGES_DETECTED=true
            else
              echo "| \`$filename\` | ✅ Passed | $mean_error | No change | ✅ |" >> "$TEST_REPORT"
            fi
          fi
        done
        
        # Validate results match expectations
        echo "" >> "$TEST_REPORT"
        echo "## Test Validation" >> "$TEST_REPORT"
        echo "" >> "$TEST_REPORT"
        
        TEST_PASSED=true
        case "$EXPECTED_RESULT" in
          "no_changes")
            if [ "$CHANGES_DETECTED" = "false" ]; then
              echo "✅ **Test PASSED**: No changes detected as expected for baseline scenario" >> "$TEST_REPORT"
            else
              echo "❌ **Test FAILED**: Changes detected but none were expected for baseline scenario" >> "$TEST_REPORT"
              TEST_PASSED=false
            fi
            ;;
          "changes")
            if [ "$CHANGES_DETECTED" = "true" ]; then
              echo "✅ **Test PASSED**: Changes detected as expected for changed scenario" >> "$TEST_REPORT"
            else
              echo "❌ **Test FAILED**: No changes detected but changes were expected for changed scenario" >> "$TEST_REPORT"
              TEST_PASSED=false
            fi
            ;;
          "mixed")
            if [ "$CHANGES_DETECTED" = "true" ]; then
              echo "✅ **Test PASSED**: Mixed results detected as expected for mixed scenario" >> "$TEST_REPORT"
            else
              echo "❌ **Test FAILED**: No changes detected but mixed results were expected" >> "$TEST_REPORT"
              TEST_PASSED=false
            fi
            ;;
        esac
        
        echo "" >> "$TEST_REPORT"
        echo "**Scenario**: ${{ matrix.scenario }}" >> "$TEST_REPORT"
        echo "**Expected**: $EXPECTED_RESULT" >> "$TEST_REPORT"
        echo "**Actual**: $([ "$CHANGES_DETECTED" = "true" ] && echo "changes_detected" || echo "no_changes")" >> "$TEST_REPORT"
        echo "**Result**: $([ "$TEST_PASSED" = "true" ] && echo "PASSED" || echo "FAILED")" >> "$TEST_REPORT"
        
        cat "$TEST_REPORT"
        
        if [ "$TEST_PASSED" = "false" ]; then
          echo "❌ Test validation failed for scenario: ${{ matrix.scenario }}"
          exit 1
        else
          echo "✅ Test validation passed for scenario: ${{ matrix.scenario }}"
        fi
    
    - name: Generate simulated production workflow output
      run: |
        # Generate what the actual visual-diff.yml workflow would produce as a PR comment
        # This simulates the production workflow output for this test scenario
        PRODUCTION_REPORT="production_output_${{ matrix.scenario }}.md"
        
        echo "# Visual Regression Test Results" > "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "**Test Run:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "## Summary" >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "| File | Status | FLIP Mean Error | Result |" >> "$PRODUCTION_REPORT"
        echo "|------|--------|-----------------|--------|" >> "$PRODUCTION_REPORT"
        
        # Initialize detailed results section
        echo "" > "detailed_results_${{ matrix.scenario }}.md"
        echo "## Detailed Results" >> "detailed_results_${{ matrix.scenario }}.md"
        echo "" >> "detailed_results_${{ matrix.scenario }}.md"
        
        # Re-process each output image to create production-style output
        summary_data=""
        for output_file in outputs/*.png; do
          if [ ! -f "$output_file" ]; then
            continue
          fi
          
          filename=$(basename "$output_file")
          golden_file="golden/$filename"
          
          echo "Processing $filename for production simulation..."
          
          if [ ! -f "$golden_file" ]; then
            # New image - simulate production output
            summary_data="$summary_data| \`$filename\` | 🆕 New | N/A | Needs acceptance |\n"
            
            # Add detailed analysis for new image (production style)
            echo "### 🆕 $filename (New Image)" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "**Status:** New image detected - no golden master exists" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "**Action Required:** This image needs to be accepted as a new golden master" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "**File Size:** $(stat -c%s "$output_file" 2>/dev/null || echo 'Unknown') bytes" >> "detailed_results_${{ matrix.scenario }}.md"
            
            # Get image dimensions
            dimensions=$(python3 -c "from PIL import Image; img = Image.open('$output_file'); print(f'{img.width}x{img.height}')" 2>/dev/null || echo 'Unknown')
            echo "**Dimensions:** $dimensions" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "" >> "detailed_results_${{ matrix.scenario }}.md"
            
            # Add image placeholders (as the production workflow would)
            echo "**New Output (Actual):**" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "![New Output](https://raw.githubusercontent.com/repo/branch/outputs/$filename)" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "" >> "detailed_results_${{ matrix.scenario }}.md"
            
            echo "To accept this new image: \`/accept-image $filename\`" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "---" >> "detailed_results_${{ matrix.scenario }}.md"
            echo "" >> "detailed_results_${{ matrix.scenario }}.md"
          else
            # Compare using FLIP (same logic as production)
            basename_no_ext=$(basename "$filename" .png)
            diff_basename="diff_${basename_no_ext}"
            
            flip_output=$(flip -r "$golden_file" -t "$output_file" -d diffs -b "$diff_basename" -v 2 -txt 2>&1)
            flip_exit_code=$?
            
            if [ $flip_exit_code -ne 0 ]; then
              summary_data="$summary_data| \`$filename\` | ❌ Error | N/A | FLIP failed |\n"
              continue
            fi
            
            # Extract FLIP statistics
            mean_error=$(echo "$flip_output" | grep "Mean:" | awk '{print $2}')
            median_error=$(echo "$flip_output" | grep "Weighted median:" | awk '{print $3}')
            q1_error=$(echo "$flip_output" | grep "1st weighted quartile:" | awk '{print $4}')
            q3_error=$(echo "$flip_output" | grep "3rd weighted quartile:" | awk '{print $4}')
            min_error=$(echo "$flip_output" | grep "Min:" | awk '{print $2}')
            max_error=$(echo "$flip_output" | grep "Max:" | awk '{print $2}')
            ppd=$(echo "$flip_output" | grep "Pixels per degree:" | awk '{print $4}')
            eval_time=$(echo "$flip_output" | grep "Evaluation time:" | awk '{print $3}')
            
            # Check if there's any meaningful difference
            is_different=$(echo "$mean_error" | awk '{if ($1 > 0.001) print "yes"; else print "no"}')
            
            if [ "$is_different" = "yes" ]; then
              summary_data="$summary_data| \`$filename\` | 🔄 Changed | $mean_error | Needs review |\n"
              
              # Add comprehensive detailed analysis for changed image (production style)
              echo "### 🔄 $filename (Changed)" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "**Status:** Visual differences detected" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "**FLIP Analysis Results:**" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Mean Error:** ${mean_error:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Median Error:** ${median_error:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **1st Quartile:** ${q1_error:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **3rd Quartile:** ${q3_error:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Min Error:** ${min_error:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Max Error:** ${max_error:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Pixels per Degree:** ${ppd:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Evaluation Time:** ${eval_time:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "**Interpretation:**" >> "detailed_results_${{ matrix.scenario }}.md"
              if [ -n "$mean_error" ]; then
                mean_threshold=$(echo "$mean_error" | awk '{
                  if ($1 > 0.1) print "High - significant visual differences"
                  else if ($1 > 0.01) print "Medium - noticeable differences"
                  else print "Low - subtle differences"
                }')
                echo "- Mean error level: $mean_threshold" >> "detailed_results_${{ matrix.scenario }}.md"
              fi
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              
              # Add image placeholders (as production workflow would)
              echo "**Golden Master (Expected):**" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "![Golden Master](https://raw.githubusercontent.com/repo/branch/golden/$filename)" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "**New Output (Actual):**" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "![New Output](https://raw.githubusercontent.com/repo/branch/outputs/$filename)" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "**Visual Difference (Highlighted Changes):**" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "![Visual Difference](https://raw.githubusercontent.com/repo/branch/diffs/diff_$filename)" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              
              echo "**Action Required:** Review the visual differences and accept if intentional" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "To accept this change: \`/accept-image $filename\`" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "---" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
            else
              summary_data="$summary_data| \`$filename\` | ✅ Passed | $mean_error | No changes |\n"
              
              # Add detailed analysis for passed image (production style)
              echo "### ✅ $filename (Passed)" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "**Status:** No significant visual differences detected" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "**FLIP Analysis Results:**" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Mean Error:** ${mean_error:-'N/A'} (below significance threshold)" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Median Error:** ${median_error:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Min Error:** ${min_error:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Max Error:** ${max_error:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Pixels per Degree:** ${ppd:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "- **Evaluation Time:** ${eval_time:-'N/A'}" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "**Result:** Image matches golden master within acceptable tolerance" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              
              # Add image placeholders (as production workflow would)
              echo "**Golden Master (Expected):**" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "![Golden Master](https://raw.githubusercontent.com/repo/branch/golden/$filename)" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "**New Output (Actual):**" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "![New Output](https://raw.githubusercontent.com/repo/branch/outputs/$filename)" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
              
              echo "---" >> "detailed_results_${{ matrix.scenario }}.md"
              echo "" >> "detailed_results_${{ matrix.scenario }}.md"
            fi
          fi
        done
        
        # Add summary data to production report
        echo -e "$summary_data" >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        
        # Append detailed results to the production report
        cat "detailed_results_${{ matrix.scenario }}.md" >> "$PRODUCTION_REPORT"
        
        # Add acceptance commands section (as production workflow would)
        echo "" >> "$PRODUCTION_REPORT"
        echo "## 🔧 Accept New Images" >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "To accept any of the new output images as golden masters, copy and paste the relevant commands below:" >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "\`\`\`" >> "$PRODUCTION_REPORT"
        
        # List any changed images that would need acceptance
        for output_file in outputs/*.png; do
          if [ -f "$output_file" ]; then
            filename=$(basename "$output_file")
            golden_file="golden/$filename"
            
            # If new or changed, add acceptance command
            if [ ! -f "$golden_file" ]; then
              echo "/accept-image $filename" >> "$PRODUCTION_REPORT"
            else
              # Check if changed (reuse FLIP check)
              flip_output=$(flip -r "$golden_file" -t "$output_file" -d diffs -b "diff_$(basename "$filename" .png)" -v 2 -txt 2>&1)
              if [ $? -eq 0 ]; then
                mean_error=$(echo "$flip_output" | grep "Mean:" | awk '{print $2}')
                is_different=$(echo "$mean_error" | awk '{if ($1 > 0.001) print "yes"; else print "no"}')
                if [ "$is_different" = "yes" ]; then
                  echo "/accept-image $filename" >> "$PRODUCTION_REPORT"
                fi
              fi
            fi
          fi
        done
        
        echo "\`\`\`" >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "> **Note:** These commands will commit the new images to the **PR branch** and update the golden master files for future comparisons." >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "## 📦 Backup Download" >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "> **Note:** Images may take a few moments to load due to CDN propagation. If images don't display immediately, please refresh the page or try again in a minute." >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "If images don't load above, download the complete results: **[Visual Test Results](https://github.com/repo/actions/runs/12345)**" >> "$PRODUCTION_REPORT"
        echo "" >> "$PRODUCTION_REPORT"
        echo "> 🧹 The temporary branch \`visual-diff-pr-123-run-456\` will be automatically cleaned up after 7 days." >> "$PRODUCTION_REPORT"
        
        echo "✅ Generated simulated production workflow output for scenario: ${{ matrix.scenario }}"
        echo ""
        echo "=== PRODUCTION WORKFLOW OUTPUT FOR SCENARIO: ${{ matrix.scenario }} ==="
        echo "This is what the actual visual-diff.yml workflow would generate as a PR comment:"
        echo ""
        cat "$PRODUCTION_REPORT"
        echo ""
        echo "=== END PRODUCTION WORKFLOW OUTPUT ==="
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: visual-diff-test-results-${{ matrix.scenario }}
        path: |
          outputs/
          diffs/
          golden/
          test_report_${{ matrix.scenario }}.md
          production_output_${{ matrix.scenario }}.md
          detailed_results_${{ matrix.scenario }}.md
        retention-days: 7

  summarize-tests:
    runs-on: ubuntu-latest
    needs: test-visual-diff-system
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v4
      with:
        pattern: visual-diff-test-results-*
        path: ./test-results
        merge-multiple: true
    
    - name: Generate comprehensive test summary
      run: |
        echo "# Visual Diff System Test Summary" > test_summary.md
        echo "" >> test_summary.md
        echo "**Test Run**: $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> test_summary.md
        echo "" >> test_summary.md
        
        # Check if all tests passed
        ALL_PASSED=true
        if [ "${{ needs.test-visual-diff-system.result }}" != "success" ]; then
          ALL_PASSED=false
        fi
        
        if [ "$ALL_PASSED" = "true" ]; then
          echo "## ✅ All Tests Passed" >> test_summary.md
          echo "" >> test_summary.md
          echo "The visual diff system is working correctly across all test scenarios:" >> test_summary.md
          echo "- **Baseline**: Correctly detects no changes when images match" >> test_summary.md
          echo "- **Changed**: Correctly detects changes when images differ" >> test_summary.md
          echo "- **Mixed**: Correctly handles mixed scenarios" >> test_summary.md
        else
          echo "## ❌ Some Tests Failed" >> test_summary.md
          echo "" >> test_summary.md
          echo "The visual diff system has issues that need to be addressed." >> test_summary.md
          echo "Check the individual test results for details." >> test_summary.md
        fi
        
        echo "" >> test_summary.md
        echo "## 🧪 Test Validation Results" >> test_summary.md
        echo "" >> test_summary.md
        echo "> This section shows the validation results for the visual diff system testing." >> test_summary.md
        echo "" >> test_summary.md
        
        # Process test validation reports from all scenarios
        for scenario in baseline changed mixed; do
          test_report="test-results/test_report_${scenario}.md"
          if [ -f "$test_report" ]; then
            echo "### Test Scenario: $scenario" >> test_summary.md
            echo "" >> test_summary.md
            # Add the test validation content
            cat "$test_report" >> test_summary.md
            echo "" >> test_summary.md
            echo "---" >> test_summary.md
            echo "" >> test_summary.md
          else
            echo "### Test Scenario: $scenario" >> test_summary.md
            echo "" >> test_summary.md
            echo "❌ **Test results not found for scenario: $scenario**" >> test_summary.md
            echo "" >> test_summary.md
            echo "---" >> test_summary.md
            echo "" >> test_summary.md
          fi
        done
        
        echo "" >> test_summary.md
        echo "## 📝 Simulated Production Workflow Output" >> test_summary.md
        echo "" >> test_summary.md
        echo "> This section shows what the actual \`visual-diff.yml\` workflow would generate as PR comments for each test scenario." >> test_summary.md
        echo "" >> test_summary.md
        
        # Process production simulation reports from all scenarios  
        for scenario in baseline changed mixed; do
          production_report="test-results/production_output_${scenario}.md"
          if [ -f "$production_report" ]; then
            echo "### Production Output for Scenario: $scenario" >> test_summary.md
            echo "" >> test_summary.md
            echo "\`\`\`" >> test_summary.md
            echo "The following is what users would see as a PR comment when using the actual visual-diff.yml workflow" >> test_summary.md
            echo "in a real CI environment with the '$scenario' test scenario data:" >> test_summary.md
            echo "\`\`\`" >> test_summary.md
            echo "" >> test_summary.md
            # Add the production workflow output
            cat "$production_report" >> test_summary.md
            echo "" >> test_summary.md
            echo "---" >> test_summary.md
            echo "" >> test_summary.md
          else
            echo "### Production Output for Scenario: $scenario" >> test_summary.md
            echo "" >> test_summary.md
            echo "❌ **Production output not found for scenario: $scenario**" >> test_summary.md
            echo "" >> test_summary.md
            echo "---" >> test_summary.md
            echo "" >> test_summary.md
          fi
        done
        
        echo "" >> test_summary.md
        echo "## 🔍 Summary" >> test_summary.md
        echo "" >> test_summary.md
        echo "This test run demonstrates:" >> test_summary.md
        echo "" >> test_summary.md
        echo "1. **Test Validation**: Whether the visual diff system correctly handles each test scenario" >> test_summary.md
        echo "2. **Production Simulation**: What users would actually see when integrating the visual-diff workflow into their CI" >> test_summary.md
        echo "" >> test_summary.md
        echo "---" >> test_summary.md
        echo "**Note**: This workflow tests the visual diff system itself. The production visual-diff workflow is separate and ready for integration with external CI systems." >> test_summary.md
        
        cat test_summary.md
        
        # Fail the workflow if tests failed
        if [ "$ALL_PASSED" = "false" ]; then
          echo "❌ Visual diff system tests failed"
          exit 1
        else
          echo "✅ Visual diff system tests passed"
        fi
    
    - name: Upload test summary
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: visual-diff-test-summary
        path: test_summary.md
        retention-days: 30
    
    - name: Comment PR with test results
      if: always() && github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read the test summary
          let report;
          try {
            report = fs.readFileSync('test_summary.md', 'utf8');
          } catch (error) {
            report = "## ❌ Visual Diff System Test Summary\n\nTest summary could not be generated due to an error.\n\n**Error:** " + error.message;
          }
          
          // Add header to distinguish from production visual diff results
          const testReport = "## 🧪 Visual Diff System Test Results\n\n" + 
            "> **This is automated testing of the visual diff system itself, not production visual regression testing.**\n" +
            "> This test validates that the visual-diff workflow works correctly and shows what users would see when integrating it into their CI.\n\n" + 
            report;
          
          // Clean up old test result comments first
          const comments = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number
          });
          
          const botTestComments = comments.data.filter(comment => 
            comment.user.type === 'Bot' && 
            comment.body.includes('🧪 Visual Diff System Test Results')
          );
          
          // Delete old test comments to avoid confusion
          for (const comment of botTestComments) {
            try {
              await github.rest.issues.deleteComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: comment.id
              });
              console.log(`Deleted old test comment: ${comment.id}`);
            } catch (error) {
              console.log(`Failed to delete comment ${comment.id}: ${error.message}`);
            }
          }
          
          // Always create a new comment with test results
          await github.rest.issues.createComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
            body: testReport
          });